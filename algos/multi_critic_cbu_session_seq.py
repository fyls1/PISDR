#coding=utf-8
import datetime

import tensorflow as tf
import numpy as np
from tensorflow.python.ops import nn_ops
from tensorflow.python.ops import rnn_cell_impl as core_rnn_cell_impl
from tensorflow.python.ops import variable_scope as vs
from tensorflow.contrib.rnn.python.ops import core_rnn_cell
import sys
from utils import *
from sl import SLCbuSessionActorOneHot
from lite_transformer import *
import os


class Actor:
    """The actor class"""

    def __init__(self, state_dim, num_actions, max_action, multi_num, name, args):
        self._name = name
        self.args = args

        with tf.variable_scope(self._name):

            self.seq_input_state = tf.placeholder(tf.float32, [None, args.seq_length, state_dim], name='seq_state')
            self.seq_input_action = tf.placeholder(tf.float32, [None, args.seq_length, num_actions], name='seq_action')
            self.seq_input_reward = tf.placeholder(tf.float32, [None, args.seq_length, 1], name='seq_action') 
            self.seq_input_time = tf.placeholder(tf.int32, [None, args.seq_length, 1], name='seq_time') 
            self._build_transformer_input(args.d_model, args.max_time, args.seq_length)
            self._build_transformer_output(args.max_time, args.d_model, args.seq_length)

            # from SL model embedding
            # self._state = tf.placeholder(dtype=tf.float32, shape=[None, state_dim], name='state')
            self._action = tf.placeholder(dtype=tf.int32, shape=[None, num_actions], name='true_action')
            self._ref_action = tf.placeholder(dtype=tf.int32, shape=[None, num_actions], name='ref_action')
            self._qvalue = tf.placeholder(dtype=tf.float32, shape=[None, multi_num], name='critic_value')
            self._weight = tf.placeholder(dtype=tf.float32, shape=[None, multi_num], name='critic_loss_weight')
            self._ratio = tf.placeholder(dtype=tf.float32, shape=[None, 1], name="ratio")
            self._var = tf.placeholder(dtype=tf.float32, shape=[], name="various")
            self.reward_ph = tf.placeholder(tf.float32, [None, 3], name='reward_click_conversion_score')
            
            constant = tf.cast(self._var, dtype=tf.float32)
            self._vars = tf.fill([tf.shape(self._state)[0], 1], constant)
            
            _action_probs = get_dnn(self._state, [512, 256, 64, max_action], [tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.softmax], "actor")
            self._action_probs = _action_probs
            self._action_choices = tf.argmax(self._action_probs, axis=-1)
            self._action_choices = tf.reshape(self._action_choices, [tf.shape(self._state)[0], 1])
            
            self.dist = tf.distributions.Normal(self._action_probs, self._vars)
            self._float_actions = tf.cast(self._action, dtype=tf.float32)
            self.log_prob = tf.reduce_sum(self.dist.log_prob(self._float_actions), axis=-1)
            

            reward_loss = tf.cast(tf.reduce_sum(self.reward_ph, axis=-1) * 10, tf.float32)

            # action_loss = tf.reduce_mean(tf.multiply(tf.nn.softmax_cross_entropy_with_logits(labels=self._action, logits=self._action_probs), reward_loss))
            action_loss = tf.losses.log_loss(labels=self._action, predictions=self._action_probs)
            qvalue_wight = tf.multiply(self._qvalue, self._weight)
            critic_loss = -tf.reduce_mean(tf.reduce_mean(qvalue_wight, axis=-1), axis=-1)

            # one stage loss
            self._loss = critic_loss + action_loss * args.bc_loss_coeff

            
            # two stage loss
            tmp_loss = tf.reduce_mean(qvalue_wight, axis=-1)
            ratio_tmp_loss = -tf.reduce_mean(tf.multiply(self._ratio, tmp_loss), axis=-1)
            if args.ac_onehot:
                ref_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self._ref_action, logits=self._action_probs))
            else:
                ref_loss = tf.losses.mean_squared_error(self._action_probs, self._ref_action)
            self._two_stage_loss = args.awac_loss_coef * ratio_tmp_loss + args.bc_loss_coeff * action_loss + args.kl_loss_coeff * ref_loss

            self._optimizer = tf.train.AdamOptimizer(learning_rate=args.lr)
            self._train_op = self._optimizer.minimize(self._loss)

            self._two_train_op = self._optimizer.minimize(self._two_stage_loss)

    def _build_transformer_input(self, d_model, max_time, seq):
        '''
        Input size: batch * seq * d_model (seq = 3, d_model generated by embedding for state action reward)
        '''

        state_hidden_input = get_dnn(self.seq_input_state, [2*d_model, d_model], [tf.nn.relu, tf.nn.relu], "seq_state_input_dnn")
        action_hidden_input = get_dnn(self.seq_input_action, [d_model], [tf.nn.relu], "seq_action_input_dnn")
        reward_hidden_input = get_dnn(self.seq_input_reward, [d_model], [tf.nn.relu], "seq_reward_input_dnn")

        # 创建Time Embedding
        self.time_embeddings_var = tf.get_variable("time_embedding_var", [max_time, d_model])
        self.time_hidden_input = tf.reshape(tf.nn.embedding_lookup(self.time_embeddings_var, self.seq_input_time), [-1, self.args.seq_length, d_model])

        
        self.state_hidden_input = tf.add(state_hidden_input, self.time_hidden_input)
        self.action_hidden_input = tf.add(action_hidden_input, self.time_hidden_input)
        self.reward_hidden_input = tf.add(reward_hidden_input, self.time_hidden_input)

        
        stack_input = tf.reshape(tf.concat([self.action_hidden_input, self.reward_hidden_input, self.state_hidden_input], axis=1), [-1, 3, seq, d_model])
        # stack_input = tf.stack([self.action_hidden_input, self.reward_hidden_input, self.state_hidden_input], axis=1)
        stack_input = tf.reshape(tf.transpose(stack_input, [0, 2, 1, 3]), [-1, 3*seq, d_model])
        self.seq_input = tf.contrib.layers.layer_norm(stack_input)
    
    def _build_transformer_output(self, max_time, d_model, seq):
        d_model = d_model
        num_layers = 2
        num_heads = 8
        dff = d_model * 2
        dropout_rate = 0.1
        self.decoder_out = decoder(self.seq_input, 3*max_time, num_layers, num_heads, [dff, d_model], dropout_rate)

        tmp_out = tf.transpose(tf.reshape(self.decoder_out, [-1, seq, 3, d_model]), [0, 2, 1, 3])
        
        self._state = tf.reshape(tmp_out[:, 2], [-1, 3*d_model])  # get decoder state


    def predict(self, sess, seq_input, time):
        return sess.run(self._action_probs, {self.seq_input_state: seq_input[-1],
                                             self.seq_input_action: seq_input[0],
                                             self.seq_input_reward: seq_input[1],
                                             self.seq_input_time: time})
    
    def cal_log(self, sess, seq_input, time, a, var):
        return sess.run(self.log_prob, {self.seq_input_state: seq_input[-1],
                                        self.seq_input_action: seq_input[0],
                                        self.seq_input_reward: seq_input[1],
                                        self.seq_input_time: time, 
                                        self._action: a, 
                                        self._var: var})

    def update(self, sess, seq_input, time, a, qvalue, weight, reward):
        _, loss = sess.run([self._train_op, self._loss], {self.seq_input_state: seq_input[-1],
                                                          self.seq_input_action: seq_input[0],
                                                          self.seq_input_reward: seq_input[1],
                                                          self.seq_input_time: time,  
                                                          self._action: a, 
                                                          self._qvalue: qvalue,
                                                          self._weight: weight, 
                                                          self.reward_ph: reward})
        return loss
    

class Critic:
    """The critic class"""

    def __init__(self, state_dim, action_dim, multi_num, name, args):
        self.args = args
        self._name = name
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.max_action = args.max_action
        self.multi_num = multi_num
        self._build_model()

    def _build_model(self):
        with tf.variable_scope(self._name):
            self.in_state = tf.placeholder(dtype=tf.float32, shape=[None, self.args.seq_length, self.state_dim], name='state')
            self._state = tf.reshape(self.in_state, [-1, self.args.seq_length * self.state_dim])
            self._action = tf.placeholder(dtype=tf.float32, shape=[None, self.action_dim], name='action')
            self._target = tf.placeholder(dtype=tf.float32, shape=[None, self.multi_num], name='target')

            q0 = get_dnn_critic(self._state, self._action, [512, 256, 64], [tf.nn.relu, tf.nn.relu, tf.nn.relu], [64, 32, 1], [tf.nn.relu, tf.nn.relu, None], "critic_dnn_00") # ctr
            q1 = get_dnn_critic(self._state, self._action, [512, 256, 64], [tf.nn.relu, tf.nn.relu, tf.nn.relu], [64, 32, 1], [tf.nn.relu, tf.nn.relu, None], "critic_dnn_01") # conver
            q2 = get_dnn_critic(self._state, self._action, [512, 256, 64], [tf.nn.relu, tf.nn.relu, tf.nn.relu], [64, 32, 1], [tf.nn.relu, tf.nn.relu, None], "critic_dnn_02") # score
            # q3 = get_dnn_critic(self._state, self._action, [512, 256, 64], [tf.nn.relu, tf.nn.relu, tf.nn.relu], [64, 32, 1], [tf.nn.relu, tf.nn.relu, None], "critic_dnn_03")
            # q4 = get_dnn_critic(self._state, self._action, [512, 256, 64], [tf.nn.relu, tf.nn.relu, tf.nn.relu], [64, 32, 1], [tf.nn.relu, tf.nn.relu, None], "critic_dnn_04")
            # q5 = get_dnn_critic(self._state, self._action, [512, 256, 64], [tf.nn.relu, tf.nn.relu, tf.nn.relu], [64, 32, 1], [tf.nn.relu, tf.nn.relu, None], "critic_dnn_05")
            # q6 = get_dnn_critic(self._state, self._action, [512, 256, 64], [tf.nn.relu, tf.nn.relu, tf.nn.relu], [64, 32, 1], [tf.nn.relu, tf.nn.relu, None], "critic_dnn_06")
            # q7 = get_dnn_critic(self._state, self._action, [32], [tf.nn.relu], [32, 1], [tf.nn.relu, None], "critic_dnn_07")
            
            self._out = tf.concat([q0, q1, q2], axis=-1)

            self._loss = tf.losses.mean_squared_error(self._out, self._target)
            self._optimizer = tf.train.AdamOptimizer(learning_rate=self.args.lr)
            self._update_step = self._optimizer.minimize(self._loss)

    def predict(self, sess, s, a):
        return sess.run(self._out, feed_dict={self.in_state: s, self._action: a})

    def update(self, sess, s, a, target):
        # print("s:",s)
        # print("a",a)
        # print("target:", target)
        _, loss = sess.run([self._update_step, self._loss], feed_dict={self.in_state: s, self._action: a,self._target: target})
        return loss


class MULTI_CRITIC(object):
    def __init__(self, args):
        # state dim from SL model embedding total sum
        state_dim, action_dim, max_action, multi_num, discount, tau, seq, d_model, max_time = args.state_dim, args.action_dim, args.max_action, args.multi_num, args.discount, args.tau, args.seq_length, args.d_model, args.max_time
        self.discount = discount
        self.args = args
        self.tau = tau
        # self.weight = [args.reward_click, args.reward_like,args.reward_follow, args.reward_comment, args.reward_forward, args.reward_hate, args.reward_play_time]
        self.weight = [args.reward_click, args.reward_conservation, args.reward_score]
        # reset graph
        tf.reset_default_graph()
        self.graph = tf.Graph() 

        self.sl_actor = SLCbuSessionActorOneHot(name="sl_actor_onehot", args=args)
        

        if self.sl_actor:
            state_dim = self.sl_actor.state_dim
        
        self.stata_dim = state_dim
        

        # self.seq_input_state = tf.placeholder(tf.float32, [None, seq, state_dim], name='seq_state')
        # self.seq_input_action = tf.placeholder(tf.float32, [None, seq, action_dim], name='seq_action')
        # self.seq_input_reward = tf.placeholder(tf.float32, [None, seq, 1], name='seq_action') 
        # self.seq_input_time = tf.placeholder(tf.float32, [None, seq, 1], name='seq_time') 
        # self._build_transformer_input(d_model, max_time, seq)
        # self._build_transformer_output(max_time, d_model, seq)

        with tf.variable_scope("one_stage"):
            # actor 输入是 transformer的输出的结果 critic 的不变
            self.actor = Actor(state_dim, action_dim, max_action, multi_num, "current_actor", args)
            self.target_actor = Actor(state_dim, action_dim, max_action, multi_num, "target_actor", args)
            if args.ac_onehot:
                self.critic = Critic(state_dim, max_action, multi_num, "current_critic", args)
                self.target_critic = Critic(state_dim, max_action, multi_num, "target_critic", args)
            else:
                self.critic = Critic(state_dim, action_dim, multi_num, "current_critic", args)
                self.target_critic = Critic(state_dim, action_dim, multi_num, "target_critic", args)

        self._build_soft_update()
    
    # def _build_transformer_input(self, d_model, max_time, seq):
    #     '''
    #     Input size: batch * seq * d_model (seq = 3, d_model generated by embedding for state action reward)
    #     '''

    #     state_hidden_input = get_dnn(self.seq_input_state, [2*d_model, d_model], [tf.nn.relu, tf.nn.relu], "seq_state_input_dnn")
    #     action_hidden_input = get_dnn(self.seq_input_action, [d_model], [tf.nn.relu], "seq_action_input_dnn")
    #     reward_hidden_input = get_dnn(self.seq_input_reward, [d_model], [tf.nn.relu], "seq_reward_input_dnn")

    #     # 创建Time Embedding
    #     self.time_embeddings_var = tf.get_variable("time_embedding_var", [max_time, d_model])
    #     self.time_hidden_input = tf.nn.embedding_lookup(self.time_embeddings_var, self.seq_input_time)

    #     self.state_hidden_input = state_hidden_input + self.time_hidden_input
    #     self.action_hidden_input = action_hidden_input + self.time_hidden_input
    #     self.reward_hidden_input = reward_hidden_input + self.time_hidden_input

    #     stack_input = tf.stack([self.reward_hidden_input, self.state_hidden_input, self.action_hidden_input], axis=1)
    #     stack_input = tf.reshape(tf.transpose(stack_input, [0, 2, 1, 3]), [-1, 3*seq, d_model])
    #     self.seq_input = tf.contrib.layers.layer_norm(stack_input)
    
    # def _build_transformer_output(self, max_time, d_model, seq):
    #     d_model = d_model
    #     num_layers = 2
    #     num_heads = 8
    #     dff = d_model * 2
    #     dropout_rate = 0.1
    #     self.decoder_out = decoder(self.seq_input, 3*max_time, num_layers, num_heads, [dff, d_model], dropout_rate)

    #     tmp_out = tf.transpose(tf.reshape(self.decoder_out, [-1, seq, 3, d_model]), [0, 2, 1, 3])
        
    #     self.action_input_state = tf.reshape(tmp_out[:, 1], [-1, 3*d_model])  # get decoder state

    def _build_soft_update(self):
        self.update_actor_target_op = []
        self.update_critic_target_op = []

        actor_var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if ("current_actor" in v.name and "one_stage" in v.name)]
        target_actor_var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if ("target_actor" in v.name and "one_stage" in v.name)]

        critic_var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if ("current_critic" in v.name and "one_stage" in v.name)]
        target_critic_var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if ("target_critic" in v.name and "one_stage" in v.name)]


        for main_var, target_var in zip(actor_var_list, target_actor_var_list):
            update_target_var = target_var.assign(self.tau * main_var + (1 - self.tau) * target_var)
            self.update_actor_target_op.append(update_target_var)

        for main_var, target_var in zip(critic_var_list, target_critic_var_list):
            update_target_var = target_var.assign(self.tau * main_var + (1 - self.tau) * target_var)
            self.update_critic_target_op.append(update_target_var)

    def select_action(self, sess, batch):
        # Todo: use cbu feature to SL get state embedding
        states = batch["states"]
        next_states = batch["next_states"]
        actions = batch["actions"]["ctr_label"]
        rewards = batch["rewards"]
        dones = batch["dones"]["done"]
        times = batch["time"]["time"]

        user_spare = states["user_spare_feature"]

        batch_size = len(user_spare)

        ctr_rewards = np.reshape(rewards["ctr_rewards"], [batch_size, self.args.seq_length, 1])
        
        curr_seq_input, next_seq_input, seq_times = self.generate_input(sess, states, next_states, actions, ctr_rewards, times, dones)
        return self.actor.predict(sess, curr_seq_input, seq_times[0])
    
    def sl_select_action(self, sess, batch):
        states = batch["states"]
        user_spare = states["user_spare_feature"]
        user_dense = states["user_dense_feature"]
        item_spare = states["item_spare_feature"]
        item_dense = states["item_dense_feature"]
        hist_spare = states["hist_spare_feature"]
        return self.sl_actor.predict(sess, user_spare, user_dense, item_spare, item_dense, hist_spare)
    
    def cal_ratio(self, sess, batch, actions):
        states = batch["states"]
        user_spare = states["user_spare_feature"]
        user_dense = states["user_dense_feature"]
        item_spare = states["item_spare_feature"]
        item_dense = states["item_dense_feature"]
        hist_spare = states["hist_spare"]
        input_state = self.sl_actor.get_state_embed(sess, user_spare, user_dense, item_spare, item_dense, hist_spare)
        log_prob_policy = self.actor.cal_log(sess, input_state, actions, 5)
        log_prob_sl_policy = self.sl_actor.cal_log(sess, user_spare, user_dense, item_spare, item_dense, hist_spare, actions, 5)
        ratios = np.clip(np.exp(log_prob_policy - log_prob_sl_policy), 0, 10)
        return ratios
    
    def total_save(self, sess, ckpt_path):
        saver = tf.train.Saver()
        saver.save(sess, save_path=ckpt_path)
        print("save total model:", ckpt_path)
    
    def total_load(self, sess, ckpt_dir):
        saver = tf.train.Saver()
        ckpt = tf.train.latest_checkpoint(ckpt_dir)
        print("load total model:", ckpt)
        saver.restore(sess, ckpt)
    
    def sl_ac_load(self, sess, sl_ckpt_dir, ac_ckpt_dir):
        print("Loading SL && AC Model...")
        sl_var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if "sl_actor_onehot" in v.name]
        ac_var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if ("one_stage" in v.name)]
        sl_saver = tf.train.Saver(var_list=sl_var_list)
        ac_saver = tf.train.Saver(var_list=ac_var_list)
        sl_ckpt = tf.train.latest_checkpoint(sl_ckpt_dir)
        ac_ckpt = tf.train.latest_checkpoint(ac_ckpt_dir)
        print("load sl model:", sl_ckpt)
        sl_saver.restore(sess, sl_ckpt)
        print("load ac model:", ac_ckpt)
        # ac_saver.restore(sess, ac_ckpt)
        print("Load Success!")

    def one_stage_save_emb(self, sess, filename):
        var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if ("one_stage" in v.name and "embedding" in v.name)]
        saver = tf.train.Saver(var_list=var_list)
        saver.save(sess, save_path=filename)
        print('Save One Stage Embedding model:', filename)
    
    def one_stage_save(self, sess, filename):
        var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if ("one_stage" in v.name)]
        saver = tf.train.Saver(var_list=var_list)
        saver.save(sess, save_path=filename)
        print('Save One Stage model:', filename)
    
    def one_stage_load(self, sess, filename):
        ckpt = tf.train.get_checkpoint_state(filename)
        var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if ("one_stage" in v.name)]
        if ckpt and ckpt.model_checkpoint_path:
            saver = tf.train.Saver(var_list=var_list)
            saver.restore(sess=sess, save_path=ckpt.model_checkpoint_path)
            print('Restore model:', ckpt.model_checkpoint_path)
    
    def sl_eval(self, sess, batch):
        user_spare = batch["user_spare_feature"]
        user_dense = batch["user_dense_feature"]
        item_spare = batch["item_spare_feature"]
        item_dense = batch["item_dense_feature"]
        hist_spare = batch["hist_spare_feature"]
        labels = batch["ctr_label"]
        loss, prob, label = self.sl_actor.evaluate(sess, user_spare, user_dense, item_spare, item_dense, hist_spare, labels)
        return loss, prob, label
    
    def sl_save(self, sess, filename):
        # print("Trainable var:", tf.trainable_variables())

        var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if "sl_actor_onehot" in v.name]
        saver = tf.train.Saver(var_list=var_list)
        saver.save(sess, save_path=filename)
        print('Save model:', filename)

    def sl_load(self, sess, filename):
        ckpt = tf.train.get_checkpoint_state(filename)
        var_list = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if "sl_actor_onehot" in v.name]
        if ckpt and ckpt.model_checkpoint_path:
            saver = tf.train.Saver(var_list=var_list)
            saver.restore(sess=sess, save_path=ckpt.model_checkpoint_path)
            print('Restore model:', ckpt.model_checkpoint_path)

    def soft_update(self, sess):
        sess.run([self.update_actor_target_op, self.update_critic_target_op])

    def sl_train(self, sess, batch):
        # print("batch info:", batch)
        # ["user_spare_feature", "user_dense_feature", "item_spare_feature", "item_dense_feature", "dynamic_item_spare_feature", "dynamic_item_dense_feature", "hist_spare_feature"]

        user_spare = batch["user_spare_feature"]
        user_dense = batch["user_dense_feature"]
        item_spare = batch["item_spare_feature"]
        item_dense = batch["item_dense_feature"]
        hist_spare = batch["hist_spare_feature"]
        labels = batch["ctr_label"]

        # print("user shape:", user_spare.shape)
        # print("user dense:", user_dense.shape)
        # print("item spare:", item_spare.shape)
        # print("item dense:", item_dense.shape)
        # print("hist spare:", hist_spare.shape)

        # print("state:", states)
        # print("action:", actions)
        _, loss = self.sl_actor.update(sess, user_spare, user_dense, item_spare, item_dense, hist_spare, labels)
        return loss
    
    def generate_input(self, sess, states, next_states, action, reward, times, dones):
        user_spare = states["user_spare_feature"]
        user_dense = states["user_dense_feature"]
        item_spare = states["item_spare_feature"] # batch * seq * 12 * size
        item_dense = states["item_dense_feature"] # batch * seq * 12 * size
        hist_spare = states["hist_spare_feature"]

        next_user_spare = next_states["user_spare_feature"]
        next_user_dense = next_states["user_dense_feature"]
        next_item_spare = next_states["item_spare_feature"]
        next_item_dense = next_states["item_dense_feature"]
        next_hist_spare = next_states["hist_spare_feature"]

        batch_size = item_spare.shape[0]
        seq_length = item_spare.shape[1]

        # output for sequence input

        current_state = np.zeros([batch_size, seq_length, self.stata_dim])
        current_action = np.zeros([batch_size, seq_length, 12])
        current_reward = np.zeros([batch_size, seq_length, 1])
        current_time = np.zeros([batch_size, 1])

        next_state = np.zeros([batch_size, seq_length, self.stata_dim])
        next_action = np.zeros([batch_size, seq_length, 12])
        next_reward = np.zeros([batch_size, seq_length, 1])
        next_time = np.zeros([batch_size, 1])

        times = np.array(times).reshape([batch_size, seq_length, 1])

        for k in range(seq_length):
            _current_state = self.sl_actor.get_state_embed(sess, user_spare, user_dense, item_spare[:, k], item_dense[:, k], hist_spare)
            _next_state = self.sl_actor.get_state_embed(sess, next_user_spare, next_user_dense, next_item_spare[:, k], next_item_dense[:, k], next_hist_spare)
            current_state[:, k] = _current_state
            next_state[:, k] = _next_state

            # if k != 0:
            #     current_action[:, k] = action[:, k-1]
            #     current_reward[:, k] = reward[:, k-1]
            
            next_action[:, k] = action[:, k]
            next_reward[:, k] = reward[:, k]
        
        for b in range(batch_size):
            for k in range(seq_length):
                if k != 0 and times[b][k][0] != 0:
                    current_action[b, k] = action[b, k-1]
                    current_reward[b, k] = reward[b, k-1]


        current_state = np.array(current_state)
        next_state = np.array(next_state)

        current_time = np.array(times).reshape([batch_size, seq_length, 1])
        next_time = current_time[1:]
        add_time = current_time[-1].reshape(1, 3, 1)
        next_time = np.concatenate([next_time, add_time], axis=0).reshape([batch_size, seq_length, 1])
        for i in range(1, len(dones)):
            if dones[i] == 0:
                next_time[i] = next_time[i-1]

        return [current_action, current_reward, current_state], [next_action, next_reward, next_state], [current_time, next_time]

    
    def train(self, sess, batch):
        states = batch["states"]
        next_states = batch["next_states"]
        actions = batch["actions"]["ctr_label"]
        critic_actions = batch["actions"]["single_ctr_label"]
        rewards = batch["rewards"]
        dones = batch["dones"]["done"]
        times = batch["time"]["time"]

        # if self.args.ac_onehot:
        #     true_actions = actions
        #     actions = np.eye(self.args.max_action)[actions].reshape([len(actions), self.args.max_action])

        user_spare = states["user_spare_feature"]
        user_dense = states["user_dense_feature"]
        item_spare = states["item_spare_feature"]
        item_dense = states["item_dense_feature"]
        hist_spare = states["hist_spare_feature"]

        next_user_spare = next_states["user_spare_feature"]
        next_user_dense = next_states["user_dense_feature"]
        next_item_spare = next_states["item_spare_feature"]
        next_item_dense = next_states["item_dense_feature"]
        next_hist_spare = next_states["hist_spare_feature"]

        batch_size = len(user_spare)

        # critic_times = np.max(times.reshape([batch_size, 3]), axis=1)
        # print("critic times:", critic_times)
        # critic_actions = []
        # for i in range(len(critic_times)):
        #     critic_actions.append(actions[i, min(2, critic_times[i])])
        # print("critic actions:", critic_actions)
        # critic_actions = np.array(critic_actions).reshape(batch_size, 12)

        weights = [self.weight for _ in range(batch_size)]

        # print("weights:", weights)

        ctr_rewards = np.reshape(rewards["ctr_rewards"], [batch_size, self.args.seq_length, 1])
        conver_rewards = np.reshape(rewards["conver_rewards"], [batch_size, self.args.seq_length, 1])
        score_rewards = np.reshape(rewards["score_rewards"], [batch_size, self.args.seq_length, 1])

        # print("ctr reward:", ctr_rewards)
        # print("conver reward:", conver_rewards)
        # print("score reward:", score_rewards)

        total_reward = np.sum(np.concatenate([ctr_rewards, conver_rewards, score_rewards], axis=-1), axis=1)

        # print("user spare:", user_spare)
        # print("total reward:", total_reward)
        # print("dones:", dones)
        
        # input_state = self.sl_actor.get_state_embed(sess, user_spare, user_dense, item_spare, item_dense, hist_spare)

        # next_input_state = self.sl_actor.get_state_embed(sess, next_user_spare, next_user_dense, next_item_spare, next_item_dense, next_hist_spare)

        curr_seq_input, next_seq_input, seq_times = self.generate_input(sess, states, next_states, actions, ctr_rewards, times, dones)

        target_q = self.target_critic.predict(sess, next_seq_input[-1], self.target_actor.predict(sess, next_seq_input, seq_times[1]))
        target_q = total_reward + self.discount * dones * target_q
        # 更新 critic
        critic_loss = self.critic.update(sess, curr_seq_input[-1], critic_actions, target_q)

        # 更新 actor
        qvalue = self.critic.predict(sess, curr_seq_input[-1], self.actor.predict(sess, curr_seq_input, seq_times[0]))
        actor_loss = self.actor.update(sess, curr_seq_input, seq_times[0], critic_actions, qvalue, weights, total_reward)

        return actor_loss, critic_loss




